# Authors:
# - Presumably written by some combination of Michael, Juan
#   Jay, and Aaron
# - Comments / Modifications by Shayne

# Purpose:
# This script combines the precomputed PDB SASA
# information with the previously generated ModBase
# SASA information to generate the Combined PDB 
# features used in Eclair These features are 
# generated by reporting the average and maxiumum 
# SASA per residue over structural sources that 
# corresponding to each UniProt using. For some
# reason this feature is reported per interaction
# rather than per protein and I am not sure why.

# Expected Outcomes:
# The output files should be updated so that it includes
# entries for the calculation of the average / max PDB
# SASA at each position for each of the interactions
# included in the input file.

# Known Bugs:
# - Dependence on static copies of the resources folder
# - Interaction order is sorted. I have confirmed there
#   was originally an assumption that the input
#   interactions should be sorted. I suspsect that
#   it is possible some features may be unavailable
#   when the interaction is not properly sorted in
#   the input. Will do testing to confirm.
# - Confused why the output here is per interaction
# - Add warnings

# Imports
from mjm_parsers import parse_dictionary_list
from collections import defaultdict
import numpy as np
import glob, os, sys

# Identify input parameter files
uniprot_info_file = sys.argv[1]
interactomes_regex = sys.argv[2]

# Precomputed SASA per PDB
# Static copy from resources
pdb_sasa_file = 'SASA_perpdb_alltax.txt'

# SASA file generated in sres_05.py
mb_sasa_file = '../modbase/SASA_modbase.txt'

# File for PDBs to exclude?
# This seems to be a mapping of PDBs that should
# be excluded for a given interaction pair
# as well as a boolean indicating whether a
# co-crystal structure for that pair exists?
#
# HEAD:
#
# UniProtA  UniProtB hasCC excludedPDBs
# A0A023PXA5   P47068   N
# A0A023PXP4   P39743   N
# A0A023PYF7   P39743   N
# A0A023PZD0   P53281   N  1KI1;2KGR;2KHN;3FIA;3HS8;3HS9;3JV3;3QBV;4IIM
# A0A023PZD0   P80667   N  1KI1;2KGR;2KHN;3FIA;3HS8;3HS9;3JV3;3QBV;4IIM
#
excluded_pdbs_file = 'excluded_pdbs.txt'

# Fixed output files
output_max_file = '/home/adr66/eclair/features/per_feature/SASA_combined_max.txt'
output_mean_file = '/home/adr66/eclair/features/per_feature/SASA_combined_avg.txt'

# Read UniProt Info file as dictionary
uniprot2info = dict((e['id'], e) for e in parse_dictionary_list(uniprot_info_file))

# Obtain a list of all the interactions
interactomes = set()
for f in glob.glob(interactomes_regex):
        print 'parsing', f, '...'
        interactomes.update(  set([tuple(sorted(l.strip().split('\t')[:2])) for l in open(f)])  )


excluded_pdbs = defaultdict(set)
for e in parse_dictionary_list(excluded_pdbs_file):
	
	# If the interaction does not have a crystal structure
	# then we don't have to worry about excluding their
	# structures.
	#
	# This has something to do with "restricting docking
	# subunits" and is linked to things that "would be used
	# to train / test classifier"
	if e['hasCC']=='N':
		continue
	
	# If there are no excluded PDBs for this pair, we can continue
	if e['excludedPDBs'] == '':
		continue
	
	# Otherwise, we sort the interaction and look up the excluded
	# PDBs from our dictionary
	#
	# Note: This is the second place I have seen any mention of
	#       sorting the interaction order. This piece of code
	#       was previously commented to imply that the interaction
	#       should already be sorted. Am afraid there may be an
	#       unstated assumption that the user has already sorted
	#       his / her input. In either case, the final version
	#       of this should be modified so that the interaction
	#       input is automatically sorted as the first step.
	interaction = tuple(sorted(tuple([e['UniProtA'], e['UniProtB']])))  #should already be sorted, but doesn't hurt to do it again
	excluded_pdbs[interaction] = set(e['excludedPDBs'].split(';'))


# Generate dictionary for mapping UniProt --> PDB --> list of Chains + SASA values?
uniprot2chains = defaultdict(lambda: defaultdict(list))

# Iterate over Previously Generated
# PDB SASA File / add to dicitonary
for e in parse_dictionary_list(pdb_sasa_file):
	sasas = np.array([ float(r) if r != 'NaN' else np.nan for r in e['UniProt_SASA'].split(';') ])
	uniprot2chains[e['UniProt']][e['PDB']].append(sasas)

# Iterate over Previously Generated
# ModBase SASA File / add to dicitonary
for e in parse_dictionary_list(mb_sasa_file):
	
	# Skip entries with low ModBal quality
	if float(e['modpipe_quality_score']) < 1.1:
		continue
	
	sasas = np.array([ float(r) if r != 'NaN' else np.nan for r in e['SASA'].split(';') ])
	uniprot2chains[e['uniprot']][e['template_pdb']].append(sasas)


# Figure out which interactions have already had
# their SASA reported in the output
already_calculated = set()
if os.path.exists(output_max_file):
	with open(output_max_file,'r') as f:
		for l in f:
			inter = tuple(l.strip().split('\t')[:2])
			already_calculated.add(inter)

# Open outputs for appending
output_max = open(output_max_file, 'a')
output_mean = open(output_mean_file, 'a')

# Iterate over each interaction
for i in interactomes:
	# Obtain UniProt IDs
	p1, p2 = i
	
	# Skip etnries where either entry is not in
	# the UniProt Info file
	if p1 not in uniprot2info or p2 not in uniprot2info:
		continue
	
	# Skip entries that have already had their
	# SASA reported
	if (p1,p2) in already_calculated or (p2,p1) in already_calculated:
		continue
	
	# Obtain SASAs for P1
	p1_sasas = []
	for pdb in uniprot2chains[p1]:
		# Skip excluded
		if pdb not in excluded_pdbs[i]:
			p1_sasas += uniprot2chains[p1][pdb]
	
	# Obtain SASAs for P2
	p2_sasas = []
	for pdb in uniprot2chains[p2]:
		# Skip excluded
		if pdb not in excluded_pdbs[i]:
			p2_sasas += uniprot2chains[p2][pdb]
			
	# Skip entries with no remaining SASA values
	# in either protein
	if len(p1_sasas)==0 and len(p2_sasas)==0:
		continue
	
	# If P1 has no values, set full result to NaN
	if len(p1_sasas) == 0:
		p1_means = [np.nan for r in range(int(uniprot2info[p1]['length']))]
		p1_max = [np.nan for r in range(int(uniprot2info[p1]['length']))]
	# Otherwise, obtain average / maximum SASA reported per residue
	# accross all sources
	else:
		# Create a masked array from all of the reported SASAs
		# The masked array just lets us easilly ignore the NaN
		# values where a given structure does not cover a position
		# when we go to calculate the average per position
		mdat = np.ma.masked_array(p1_sasas, np.isnan(p1_sasas))
		
		# Generate mean SASA per position over all sources / set
		# NaN for any positions with no information
		p1_means = np.mean(mdat, axis=0)
		p1_means = [p1_means.data[r] if p1_means.mask[r]==False else np.nan for r in range(len(p1_means.data))]
		
		# Generate max SASA per position over all sources / set
		# NaN for any positions with no information
		p1_max = np.max(mdat, axis=0)
		p1_max = [p1_max.data[r] if p1_max.mask[r]==False else np.nan for r in range(len(p1_max.data))]
	
	# If Pw has no values, set full result to NaN
	if len(p2_sasas) == 0:
		p2_means = [np.nan for r in range(int(uniprot2info[p2]['length']))]
		p2_max = [np.nan for r in range(int(uniprot2info[p2]['length']))]
	# Otherwise, obtain average / maximum SASA reported per residue
	# accross all sources
	else:
		# Create a masked array from all of the reported SASAs
		# The masked array just lets us easilly ignore the NaN
		# values where a given structure does not cover a position
		# when we go to calculate the average per position
		mdat = np.ma.masked_array(p2_sasas, np.isnan(p2_sasas))
		
		# Generate mean SASA per position over all sources / set
		# NaN for any positions with no information
		p2_means = np.mean(mdat, axis=0)
		p2_means = [p2_means.data[r] if p2_means.mask[r]==False else np.nan for r in range(len(p2_means.data))]
		
		# Generate max SASA per position over all sources / set
		# NaN for any positions with no information
		p2_max = np.max(mdat, axis=0)
		p2_max = [p2_max.data[r] if p2_max.mask[r]==False else np.nan for r in range(len(p2_max.data))]
	
	# Skip entries where the length of either of the UniProts
	# based on SASA features does not match the expected
	# length of the UniProt based on the UniProt Info
	# Should throw a warning
	if len(p1_means) != int(uniprot2info[p1]['length']) or len(p2_means) != int(uniprot2info[p2]['length']):
		continue
	
	# Write output files
	# Not sure why this has to be a pair-wise
	# feature rather than a protein specific
	# feature?
	#
	# HEAD:
	#
	# P1  P2 P1AvgSASA   P2AvgSASA
	# P1  P2 P1MaxSASA   P2MaxSASA
	output_max.write('%s\t%s\t%s\t%s\n' %(p1, p2, ';'.join(['%.2f' %(res) for res in p1_max]), ';'.join(['%.2f' %(res) for res in p2_max])))
	output_mean.write('%s\t%s\t%s\t%s\n' %(p1, p2, ';'.join(['%.2f' %(res) for res in p1_means]), ';'.join(['%.2f' %(res) for res in p2_means])))

# Close Outputs
output_max.close()
output_mean.close()
