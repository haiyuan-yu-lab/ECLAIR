# Authors:
# - Presumably written by some combination of Michael, Juan
#   Jay, and Aaron
# - Comments / Modifications by Shayne

# Purpose:
# This script parses the spit DCA outputs generated by
# batch_dca.py and plit_dca_mi_di.py to calculate the
# final ten features used in Eclair. The script skips
# recalculating these features for any interactions
# that have previously had them calculated.

# Expected Outcomes:
# All interactions included in the input file should
# have the ten DCA DI and MI features calculated and
# should have corresponding entries in the relevant
# per_features output file. The features, five for
# each DI and MI values are...
#
# 1) Max - The highest score for each residue
#
# 2) Mean - The average score for each residue
#
# 3) Top10 - The average of of the 10 highest scores
#            for each residue
#
# 4) Max_localZ - The highest score for each reisude
#                 Z normalized by the rest of the protein
#
# 5) Max_globalZ - The highest score for each residue
#                  Z normalized by the rest of the alignment

# Known Bugs:
# - No known bugs, but concerns including...
# - Warnings that should be thrown on edge cases

# Imports
import os, glob, sys
import pandas as pd
import numpy as np
from mjm_parsers import parse_dictionary_list

#priority_interactions = set(['_'.join(sorted(l.strip().split('\t')[:2])) for l in open('../../features/per_feature/ires50.txt')][0:])

# Generate Input Directories for DI and MI components
# of the DCA outputs
data_dir_pref = '/home/adr66/eclair/data/sca/'
data_dirs = [data_dir_pref + 'di/', data_dir_pref + 'mi/']
file_exts = ['.di', '.mi']
abrevs = ['DDI', 'DMI']

# Identify input Uniprot info file
uniprot_info_file = sys.argv[1]

# Fixed Output Direcoties
output_dir = '/home/adr66/eclair/features/per_feature/'

# Prioritize interactions from the input Interaciton
# file
priority_interactions = set(['_'.join(sorted(l.strip().split('\t')[:2])) for l in open(sys.argv[2])])

# Generate final feature files for each the DI
# and MI DCA values
for i in range(len(data_dirs)):
	
	# Select appropriate outputs / file information
	data_dir = data_dirs[i]
	file_ext = file_exts[i]
	abrev = abrevs[i]
	
	# Find ALL of the DCA DI or MI files that exist
	# sort them with outputs relevant to the input
	# interactions first
	sca_files = sorted(glob.glob(os.path.join(data_dir, '*'+file_ext)), 
                   key=lambda f: os.path.basename(f)[:-1*len(file_ext)] in priority_interactions, reverse=True)
	
	# Generate mapping for output files names
	out_files = {'max': '%s_max.txt' %(abrev),
				 'top10': '%s_top10.txt' %(abrev),
				 'mean': '%s_mean.txt' %(abrev),
				 'max_localZ': '%s_max_localZ.txt' %(abrev),
				 'max_globalZ': '%s_max_globalZ.txt' %(abrev)}
	
	# Map UniProt IDs to Sequence
	uniprot2aaseq = dict((e['id'], e['sequence']) for e in parse_dictionary_list(uniprot_info_file))
	
	#-----------------------------------------------------------------
	
	#create and clear files for later appending
	#for f in out_files.values():
	#	output = open(os.path.join(output_dir, f), 'w')
	#	output.close()
	
	# Generate list of all interactions that have already
	# had these features calculated (specifically based on
	# the DDI_max.txt or DMI_max.txt)
	already_computed = set(['_'.join(l.strip().split('\t')[:2]) for l in open(os.path.join(output_dir,out_files['max']))])
	
	#-----------------------------------------------------------------
	
	# Generate batches of size 200 for parsing
	# ouputs / calculating aggregate features
	batch_size = 200
	batches = [sca_files[i*batch_size:(i+1)*batch_size] for i in range(0,len(sca_files)/batch_size+1)]
	
	# Iterate over each batch
	for batch_num, sca_batch in enumerate(batches):
		
		# Create dictionary for storing batch results
		# for each aggregate feature
		batch_results = {key: {} for key in out_files}
		
		# Iterate over each of the individual DI or MI files in the batch
		for num, f in enumerate(sca_batch):
			
			print num+1+batch_num*batch_size,'/', len(sca_files), f
			
			# Obtain UniProt IDs for the interaction
			u1, u2 = os.path.basename(f).split('.')[0].split('_')[:2]
			
			# Skip homodimers
			if u1==u2:continue
			
			# Skip interactions that have already been calculated
			if '_'.join([u1,u2]) in already_computed:
				continue
			
			# Skip interactions that do not show up in the UniProt
			# Infor File (How is this possible?)
			# This should throw a warning
			if u1 not in uniprot2aaseq or u2 not in uniprot2aaseq:
				continue
			
			# Obtain the length of each member of the interaction
			len1, len2 = len(uniprot2aaseq[u1]), len(uniprot2aaseq[u2])
			
			# Read the DI or MI file (this time as a DataFrame?)
			# Read the DF, set the two positions as the indices, unstack
			# it creating a matrix as follows...
			#
			# HEAD:
			#
			#			Score
			#	resB	2		3		4		5		6
			#	resA
			#	1		1.4	1.0	1.2	1.1	1.1
			#	2		NaN	1.2	1.4	1.1	1.2
			#	3		NaN	NaN	1.1	1.1	1.0
			#	4		NaN	NaN	NaN	1.0	1.4
			#	5		NaN	NaN	NaN	NaN	1.0
			#
			df = pd.read_table(f ,header=None, names=['resA','resB','Score']).set_index(['resA','resB']).unstack()
			
			# Ensure that there is consistency between the lengths from the
			# SCA alignments and the cummulative length of the individual
			# UniProt sequences
			#
			# Apparently there are 8/923 interactions where this was the case
			# in some test case
			#
			# This should definitely throw a warning
			tot_len = df.shape[0] + 1
			if tot_len != len1 + len2:
				print 'Protein lengths unexpected. Skipping ', f
				continue
			
			# Re-index DataFrame (basically just removes the names of
			# all of the indices)
			df.columns = df.columns.levels[1].values
			df.index = df.index.values
			
			# Add NaN row at the bottom of the DF and NaN column
			# to the left of DF
			df[1] = np.nan
			df = df[sorted(df.columns)]
			df.loc[len(df)+1] = np.nan
			
			# Parse DF into a Matrix where each row corresponds with
			# one position in P1 and each column corresponds with one
			# position in P2 (basically just removes all of the
			# intra-protein position pairs / keeps all the inter-protein
			# relationships).
			#
			#             all the rows in P1
			#                   |
			#                 all the columns in P2
			#                   |     |
			#                   v     v
			npdat = df.values[:len1, len1:]
			
			# Calculate the Features			
			
			# MAX
			# Save highest score for each residue in each protein
			p1_max = np.nanmax(npdat, axis=1)
			p2_max = np.nanmax(npdat, axis=0)
			batch_results['max'][(u1, u2)] = (p1_max, p2_max)
			
			# Helper function to calculate mean
			# ignoring NaN values
			def nanmean(arr, axis=None):
				return np.mean(arr[np.isnan(arr) == False].reshape(arr.shape), axis=axis)
			
			# Helper function to calculate std
			# ignoring NaN values
			def nanstd(arr, axis=None):
				return np.std(arr[np.isnan(arr) == False].reshape(arr.shape), axis=axis)
			
			# MEAN
			# Save the mean score for each residue in each protein
			p1_mean = nanmean(npdat, axis=1)
			p2_mean = nanmean(npdat, axis=0)
			batch_results['mean'][(u1, u2)] = (p1_mean, p2_mean)
			
			
			# TOP10
			# Save the mean of the top 10 highest scores for each
			# residue in each protein
			p1_top10 = np.mean(np.sort(npdat, axis=1)[:,-10:], axis=1)
			p2_top10 = np.mean(np.sort(npdat, axis=0)[-10:,:], axis=0)
			batch_results['top10'][(u1, u2)] = (p1_top10, p2_top10)
			
			# MAX_LOCALZ
			# Save the highest score for each residue in each protein
			# Z normalized by the rest of the scores for that protein
			p1_localZ = (p1_max - p1_mean) / nanstd(npdat, axis=1)
			p2_localZ = (p2_max - p2_mean) / nanstd(npdat, axis=0)
			batch_results['max_localZ'][(u1, u2)] = (p1_localZ, p2_localZ)
			
			# MAX_GLOBALZ
			# Save the highest score for each residue in each protein
			# Z normalized by the scores of the entire alignment
			p1_globalZ = (p1_max - nanmean(npdat)) / nanstd(npdat)
			p2_globalZ = (p2_max - nanmean(npdat)) / nanstd(npdat)
			batch_results['max_globalZ'][(u1, u2)] = (p1_globalZ, p2_globalZ)		
		
		
		# Write Outputs
		print '... writing batch output for %i entries' %(len(sca_batch))
		
		# Iterate over each feature output file
		for feature in out_files:
			# Open output for appending
			output = open(os.path.join(output_dir, out_files[feature]), 'a')
			
			# Iterate over each interaction
			for interaction in batch_results[feature]:
				# Obtain UniProt IDs
				u1, u2 = interaction
				
				# Generate lists of feature values for each UniProt
				v1, v2 = batch_results[feature][interaction]
				
				# Write formatted feature output
				#
				# HEAD:
				#
				# P1     P2       Feature Per Res (P1)                Feature Per Res (P2)
				# F4K567 O64852   2.056926;0.538821;0.269124;1.101526 0.492727;0.099668;0.092347;0.091123
				#
				output.write('%s\t%s\t%s\t%s\n' %(u1, u2, ';'.join(['%.6f' %(res) for res in v1]), ';'.join(['%.6f' %(res) for res in v2])))
			
			# Close Output
			output.close()
